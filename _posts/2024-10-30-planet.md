---
title: 'Temp'
date: 2024-10-30
permalink: /posts/2024/10/planet/
tags:
 - RL
 - Model-based
---

## The ability to foresee the future
Have you ever thought about what if you can foresee the future? If you know what will happen when you do something, definitely you can decide your action much more easily. For example, you can avoid an action if you already know it may harm you. Or you can select the best action among the actions you can take. This kind of foresight is not only appealing to humans but also to autonomous agents.

To allow the autonomous agent to move by itself, we can use the reinforcement learning approach. Previously, to get samples which include rewards for training, we should interact with the environment. If then, the environment can't be restored. Even though you realize that the agent performed the bad action (got a low reward), the agent can't return back to the previous step and has to keep moving from the bad state.

However what if the agent can know what will happen in the next step? Greedily speaking, it can just perform the action that will give it the highest reward. Or if it considers further future, it can simulate many possible routes in the imagination and just follow the route that will give it the highest cumulative reward. We can call this process as a **planning**.

![Planning process]()

For example, as you can see in the above figure, planning allows the agent to move through the safer and better path.

Now we know we can allow the autonomous agent to move by planning if it can predict the future. Then, how can it predict the future?

## What is the world model?
While the definition of the world model is slightly different by literatures, generally it includes the **transition model** (a.k.a. dynamics model) and the **reward model**. 

The **transition model** predicts how the state of the agent will be changed if it performs the given action.   
The **reward model** predicts what reward the agent will get by performing the given action.

![Figure explaining worldmodel]()

If the environment is artificial and simple, then it might be able to hand-write the world model. For example, the board game environments such as a chess are easy to predict what will happen after performing an action. But more natural and complex envrionment might be unable to hand-write all transitions and rewards corresponding to every states.

Fortunately, thanks to the rise of deep neural network, we have been able to train the world model with deep learning methods! But still we need an accurate world model for planning (especially if we want to predict farther then model should be more accurate!). So many researchers have been trying to train a world model **more accurately** and also to train a world model of **more complex environment**.

Nowadays, there are diverse architectures designing the world model. In this post, I would like to introduce one of them, RSSM which uses both stochastic and deterministic latent dynamics model to accurately predict the state from pixel based observation.

## Recurrent State Space Model

### Latent dynamics model
As I already mentioned, learning a world model for more complex environments is harder than for the simpler one.

## PlaNet

## Experiment Results

## Recent works
There are also other algorithms using RSSM as a world model. They are the Dreamer series and Director. Dreamer improves the performance of agents via training a policy network with imagined data generated by the world model, and Director trains two policies in a hierarchical manner to tackle longer and more complex tasks. I'm looking forward to introducing these algorithms someday.

<!-- <h2 style="display:inline-block">Block</h2> that's right

<details>
<summary><h2 style="display:inline-block"> Why do we need planning?</h2> </summary>


</details> -->