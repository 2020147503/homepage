---
title: 'Temp'
date: 2024-10-30
permalink: /posts/2024/10/planet/
tags:
 - RL
 - Model-based
---

## The Ability to Foresee The Future
Have you ever thought about having a superpower to forsee the future? If you know what will happen when you do something,  you definitely can decide your action much more easily. For example, you can avoid an action if you already know it may harm you. Or you can select the best action among the actions you can take. This kind of foresight is not only appealing to humans but also to autonomous agents.

To allow the autonomous agent to move by itself, we can use the reinforcement learning approach. Previously, to get samples which include rewards for training, we should interact with the environment. If then, the environment can't be restored. Even though you realize that the agent performed a bad action (got a low reward), the agent can't return back to the previous step and has to keep moving from the bad state.

However, what if the agent can know what will happen in the next step? Greedily speaking, it can just perform the action that leads to the highest reward. Or if it considers further future, it can simulate many possible routes in the imagination and just follow the route that will give it the highest cumulative reward. We can call this process as **planning**.

![Planning process](/images/planet_planning.gif)

For example, as you can see in the above figure, planning allows the agent to move through the safer and better path.

Now we know we can allow the autonomous agent to move by planning if it can predict the future. Then, how can it predict the future?

## What's a World Model?
While the definition of the world model is slightly different by literatures, generally it includes the **transition model** (a.k.a. dynamics model) and the **reward model**. 

The **transition model** predicts how the state of the agent changes after it performs a given action.   
The **reward model** predicts what reward the agent will receive by performing the given action.

![Figure explaining worldmodel](/images/planet_worldmodel.png)

If the environment is artificial and simple, then it might be easy to manually design the world model. For example, the board game environments such as a chess are easy to predict what will happen after performing an action. But more natural and complex envrionment like the real world might be hard or even impossible to design manually all transitions and rewards corresponding to every states. 

Fortunately, thanks to the rise of deep neural network, we have been able to train the world model with deep learning methods! But still we need an accurate world model for planning (especially if we want to predict farther then model should be more accurate!). So, many researchers have been trying to train a world model **more accurately** and also to train a world model of **more complex environment**.

Nowadays, there are diverse architectures designing the world model. In this post, I would like to introduce one of them, RSSM which uses both stochastic and deterministic latent dynamics model to accurately predict the state from pixel-based observation.

## Recurrent State Space Model

As I already mentioned, learning a world model for more complex environments is harder than for the simpler one. I will say that pixel-based observation is more complex than knowing true state due to their high dimensionality and the partially observability.

![state and pixel observation](/images/planet_observation_and_state.png)
*<small>If we observe something with the camera, then we may not be able to capture the every information of the true state. Also the dimension of a image is usally higher than one of the information what we interest.</small>*

### Latent Dynamics Model

To reduce the complexity of the environment even a little bit, we can encode the observations into the latent space and predict the future states upon the latent space. This is the concept of the latent dynamics model. Now we need one more model to build a world model. It's the **observation model** which encodes the observations to the latent state.

![observatoin model](/images/planet_observation_model.png)
*<small>Observation model encodes the observation into the latente state</small>*
### Sequential Model

To approximate the latent state as close as the true state, we can use the information of state in the history. Since the agent's states are sequential in terms of the time, we can consider to use the sequential model to build a world model. 

![example of the sequential model]()

Generally, the environment would not be deterministic. It means that even though the agent performs the exact same action on the same state, there is a probability to lead it to the different state. So, It's more natural that the transition model predict the probability of the next state than predict the next state directly.

![stochasticity]()

But if we use that stochastic transition model in the sequential model, it may difficult for the sequential model to remember the information from history.

![stochastic and deterministic]()

RSSM uses both stochastic and deterministic transition models to mitigate this limitation.

![RSSM]()



## PlaNet

## Experiment Results

## Recent Works
There are also other algorithms using RSSM as a world model. They are the Dreamer series and Director. Dreamer improves the performance of agents via training a policy network with imagined data generated by the world model, and Director trains two policies in a hierarchical manner to tackle longer and more complex tasks. I'm looking forward to introducing these algorithms someday.

<!-- <h2 style="display:inline-block">Block</h2> that's right

<details>
<summary><h2 style="display:inline-block"> Why do we need planning?</h2> </summary>


</details> -->